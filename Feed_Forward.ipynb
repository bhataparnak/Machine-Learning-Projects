{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feed_Forward.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-iLrdCBCna3"
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ-Eo2eaCzER"
      },
      "source": [
        "#Sigmoid Activation Function\n",
        "def sigmoid(x, derive=False):\n",
        "    if derive:\n",
        "        return x * (1 - x)\n",
        "    return 1 / (1 + np.exp(-x))"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQQHEwg9C8GZ",
        "outputId": "b5c46499-c8ef-4dac-fb3a-dd1cd9eb188a"
      },
      "source": [
        "#Define the Data set OR logic\n",
        "#Initialize the 8 samples of the OR function.\n",
        "X = np.array([\n",
        "    [1, 1, 1, 1],\n",
        "    [1, 1, -1, 1],\n",
        "    [1, 0, 1, 1],\n",
        "    [1, -1, -1, 1],\n",
        "    [-1, 1, 1, 1],\n",
        "    [-1, 1, -1, 1],\n",
        "    [-1, -1, 1, 1],\n",
        "    [-1, -1, -1, 1],\n",
        "])\n",
        "indices_one = X == -1\n",
        "X[indices_one] = 0 # replacing 1s with 0s\n",
        "print(X)\n",
        "#Define the labels of each sample.\n",
        "y = np.array([[1],\n",
        "              [1],\n",
        "              [1],\n",
        "              [1],\n",
        "              [1],\n",
        "              [1],\n",
        "              [1],\n",
        "              [-1]\n",
        "             ])\n",
        "\n",
        "indices_one = y == -1\n",
        "y[indices_one] = 0 # replacing 1s with 0s\n",
        "print(y)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 1 1 1]\n",
            " [1 1 0 1]\n",
            " [1 0 1 1]\n",
            " [1 0 0 1]\n",
            " [0 1 1 1]\n",
            " [0 1 0 1]\n",
            " [0 0 1 1]\n",
            " [0 0 0 1]]\n",
            "[[1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zdd6KielEfLX"
      },
      "source": [
        "# Define a learning rate\n",
        "lr = 0.1\n",
        "# Define the number of epochs for learning\n",
        "epochs = 3\n",
        "\n",
        "# Initialize the weights with random numbers\n",
        "w01 = np.random.random((len(X[0]), 4))   #Initialize the hidden layer weights with ones. Size of hidden layer is 4.\n",
        "w12 = np.random.random((4, 1))           #Initialize the output layer weights with ones. Size of output layer is 1.\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhVQi-EQE-qu",
        "outputId": "b49bf2f5-a3d7-4eb3-f997-32f6a9b11448"
      },
      "source": [
        "# Start feeding forward and backpropagate *epochs* times.\n",
        "for epoch in range(epochs):\n",
        "    # Feed forward\n",
        "    z_h = np.dot(X, w01) #Calculate the input of the hidden layer zh using matrix multiplication.\n",
        "    #print(z_h)\n",
        "    a_h = sigmoid(z_h)   #Calculate the output of the hidden layer ah using sigmoid activation function.\n",
        "    #print(a_h)\n",
        "\n",
        "    z_o = np.dot(a_h, w12)   #Calculate the input of the output layer zo using matrix multiplication.\n",
        "    #print(z_o)\n",
        "    a_o = sigmoid(z_o)      #Calculate the output of the output layer ao using sigmoid activation function.\n",
        "    #print(a_o)\n",
        "\n",
        "    # Calculate the error\n",
        "    a_o_error = ((1 / 2) * (np.power((a_o - y), 2)))  # Calculate the error of the output neuron ao using squared error function.\n",
        "    #print(a_o_error)\n",
        "\n",
        "    # Backpropagation\n",
        "    ## Output layer\n",
        "    delta_a_o_error = a_o - y    #Calculate the derivate of the error of the output layer.\n",
        "    #print(delta_a_o_error)\n",
        "    delta_z_o = sigmoid(a_o,derive=True)  #Calculate the derivate of the sigmoid function of the output layer.\n",
        "    #print(delta_z_o)\n",
        "    delta_w12 = a_h    #Calculate the derivate of the input of the output layer zo with respect to the weights w.\n",
        "    #print(a_h)\n",
        "    delta_output_layer = np.dot(delta_w12.T,(delta_a_o_error * delta_z_o))  # Calculate the update matrix for the output layer using matrix multiplication and hadamard product.\n",
        "    #print(delta_output_layer)\n",
        "\n",
        "    ## Hidden layer\n",
        "    delta_a_h = np.dot(delta_a_o_error * delta_z_o, w12.T)  #Calculate the derivate of the Error function E with respect to the output of the hidden layer ah.\n",
        "    #print(delta_a_h)\n",
        "    delta_z_h = sigmoid(a_h,derive=True)  #Calculate the derivate of the sigmoid of the hidden layer.\n",
        "    #print(delta_z_h)\n",
        "    delta_w01 = X   #Calculate the derivate of the input of the hidden layer zh with respect to the weight matrix w01.\n",
        "    #print(X)\n",
        "    delta_hidden_layer = np.dot(delta_w01.T, delta_a_h * delta_z_h) #Calculate the update matrix for the hidden layer using matrix multiplication and hadamard product.\n",
        "    #print(delta_hidden_layer)\n",
        "\n",
        "    w01 = w01 - lr * delta_hidden_layer\n",
        "    w12 = w12 - lr * delta_output_layer\n",
        "    print(w01)\n",
        "    print(w12)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.06385209 0.09828322 0.62829311 0.62183754]\n",
            " [0.52752095 0.93706289 0.92957938 0.16109411]\n",
            " [0.74522796 0.35365902 0.1055096  0.33429338]\n",
            " [0.96567855 0.52858341 0.41518875 0.51060044]]\n",
            "[[0.07433442]\n",
            " [0.4339805 ]\n",
            " [0.71333701]\n",
            " [0.37330689]]\n",
            "[[0.06400836 0.09942675 0.62990356 0.6227455 ]\n",
            " [0.52764484 0.93788195 0.9309297  0.16207118]\n",
            " [0.74535412 0.35477809 0.10745507 0.3353267 ]\n",
            " [0.96573524 0.52915359 0.41600555 0.5112246 ]]\n",
            "[[0.08688154]\n",
            " [0.44605262]\n",
            " [0.72589386]\n",
            " [0.38487862]]\n",
            "[[0.06418173 0.10054108 0.63145583 0.62363204]\n",
            " [0.52778183 0.93867695 0.93222662 0.16302311]\n",
            " [0.74549411 0.35586926 0.10933259 0.33633675]\n",
            " [0.9657873  0.52964381 0.41668266 0.51177708]]\n",
            "[[0.09832664]\n",
            " [0.45710974]\n",
            " [0.73743114]\n",
            " [0.39546889]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}